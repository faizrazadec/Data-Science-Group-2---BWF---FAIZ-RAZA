{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6056f60-fdc3-4d5e-8c43-a7296fda6293",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d0a407-0bff-4200-95ca-4eea0f13598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ddd1a-3a84-4277-bbaa-771418420154",
   "metadata": {},
   "source": [
    "# Defining Functions\n",
    "\n",
    "- The `sigmoid function` is an activation function that introduces non-linearity. It outputs values between 0 and 1.\n",
    "- The `sigmoid_derivative` function computes the derivative of the sigmoid function, used in the backward pass for gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fedbbaf-0c57-46a6-bc1f-ef935c73a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125ef21-aa0b-4892-9324-8c2029827cca",
   "metadata": {},
   "source": [
    "- The mean_squared_error function calculates the mean squared error between the true and predicted values. This is the loss function used to measure the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a83a9-4e1c-4334-a98e-a778d90ac86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6d1e5-fb9c-4932-ad7f-25926f7bae6d",
   "metadata": {},
   "source": [
    "# Neural Network Class\n",
    "### Initialization\n",
    "\n",
    "- `__init__` method initializes the neural network with the given architecture and learning rate.\n",
    "- The `weights` and `biases` for the connections between the input layer and hidden layer (weights_input_hidden and bias_hidden) and the hidden layer and output layer (weights_hidden_output and bias_output) are randomly initialized.\n",
    "\n",
    "### Forward Propagation\n",
    "- forward method calculates the outputs for each layer in the network.\n",
    "- `self.hidden_input` is the weighted sum of inputs plus the bias for the hidden layer.\n",
    "- `self.hidden_outpu`t applies the sigmoid activation function to self.hidden_input.\n",
    "- `self.final_input` is the weighted sum of the hidden layer outputs plus the bias for the output layer.\n",
    "- `self.final_output` is the final output of the network. In regression tasks, a linear activation function (identity function) is used, so self.final_output is simply self.final_input.\n",
    "\n",
    "### Backward Propagation\n",
    "- `backward` method calculates the gradients of the loss with respect to the weights and biases and updates them.\n",
    "- error is the difference between the predicted output and the true output.\n",
    "- `d_output` is the gradient of the loss with respect to the output (identity function derivative is 1).\n",
    "- `error_hidden_layer` is the backpropagated error for the hidden layer.\n",
    "- `d_hidden_layer` is the gradient of the loss with respect to the hidden layer output, calculated using the sigmoid derivative.\n",
    "- The weights and biases are updated using gradient descent by subtracting the product of the learning rate and the calculated gradients.\n",
    "\n",
    "### Training the Network\n",
    "- `train` method trains the network for a specified number of epochs.\n",
    "- For each `epoch`, it performs forward propagation, backward propagation, and updates the weights and biases.\n",
    "- Every `1000 epochs`, it prints the current loss to monitor training progress.\n",
    "- predict method uses the trained network to make predictions on new input data by performing a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93a4af20-386d-495f-a429-2016fc6792a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.bias_hidden = np.random.rand(self.hidden_size)\n",
    "        self.bias_output = np.random.rand(self.output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.final_output = self.final_input  # Linear activation for regression\n",
    "        \n",
    "        return self.final_output\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Backward pass\n",
    "        error = output - y\n",
    "        d_output = error  # Linear activation derivative is 1\n",
    "        \n",
    "        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)\n",
    "        d_hidden_layer = error_hidden_layer * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output -= self.hidden_output.T.dot(d_output) * self.learning_rate\n",
    "        self.bias_output -= np.sum(d_output, axis=0) * self.learning_rate\n",
    "        \n",
    "        self.weights_input_hidden -= X.T.dot(d_hidden_layer) * self.learning_rate\n",
    "        self.bias_hidden -= np.sum(d_hidden_layer, axis=0) * self.learning_rate\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = mean_squared_error(y, output)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "                \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab731f9-a129-49f2-8752-9ec40c2de610",
   "metadata": {},
   "source": [
    "# Let's Try the Example over the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e14771af-6334-4594-9fd2-e862ad13dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.281927743424347\n",
      "Epoch 100, Loss: 3.0604229517224355\n",
      "Epoch 200, Loss: 0.6250204567000118\n",
      "Epoch 300, Loss: 0.1309935465549913\n",
      "Epoch 400, Loss: 0.061116686063026435\n",
      "Epoch 500, Loss: 0.04548076003605538\n",
      "Epoch 600, Loss: 0.03868937335680722\n",
      "Epoch 700, Loss: 0.03440414886641851\n",
      "Epoch 800, Loss: 0.03124107655478059\n",
      "Epoch 900, Loss: 0.02869268536480819\n",
      "Predictions:\n",
      "[[0.18049316]\n",
      " [1.80234796]\n",
      " [3.99901613]\n",
      " [6.19483711]\n",
      " [7.84856184]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0], [1], [2], [3], [4]])\n",
    "y = np.array([[0], [2], [4], [6], [8]])\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "nn.train(X, y, epochs)\n",
    "\n",
    "predictions = nn.predict(X)\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
